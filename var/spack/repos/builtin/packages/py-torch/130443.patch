From 04fc29b4a7709366ab03d15d84d3cf4d7083c82f Mon Sep 17 00:00:00 2001
From: Harmen Stoppels <me@harmenstoppels.nl>
Date: Wed, 10 Jul 2024 18:05:00 +0200
Subject: [PATCH] flash_attn: limit parallelism in build

The flash attention cuda files require an enormous amount of memory and
on most systems cause OOM errors when compiling with -j $(nproc).

Compiling those files with a ninja pool fixes the problem.
---
 aten/src/ATen/CMakeLists.txt | 8 ++++++--
 caffe2/CMakeLists.txt        | 4 ++++
 2 files changed, 10 insertions(+), 2 deletions(-)

diff --git a/aten/src/ATen/CMakeLists.txt b/aten/src/ATen/CMakeLists.txt
index bf425af5fa..91799773e3 100644
--- a/aten/src/ATen/CMakeLists.txt
+++ b/aten/src/ATen/CMakeLists.txt
@@ -177,9 +177,13 @@ file(GLOB mem_eff_attention_cuda_cu "native/transformers/cuda/mem_eff_attention/
 file(GLOB mem_eff_attention_cuda_kernels_cu "native/transformers/cuda/mem_eff_attention/kernels/*.cu")
 file(GLOB mem_eff_attention_cuda_cpp "native/transformers/cuda/mem_eff_attention/*.cpp")
 
+# flash_attn for cuda requires excessive memory, so restrict parallelism in ninja builds
+add_library(flash_attention_cuda OBJECT)
+set_property(GLOBAL PROPERTY JOB_POOLS flash_attention_pool=3)
+set_property(TARGET flash_attention_cuda PROPERTY JOB_POOL_COMPILE flash_attention_pool)
+
 if(USE_FLASH_ATTENTION)
-  list(APPEND native_transformers_cuda_cu ${flash_attention_cuda_cu})
-  list(APPEND native_transformers_cuda_cu ${flash_attention_cuda_kernels_cu})
+  target_sources(flash_attention_cuda PRIVATE ${flash_attention_cuda_cu} ${flash_attention_cuda_kernels_cu})
   list(APPEND native_transformers_cuda_cpp ${flash_attention_cuda_cpp})
   list(APPEND FLASH_ATTENTION_CUDA_SOURCES ${flash_attention_cuda_cu} ${flash_attention_cuda_kernels_cu})
   list(APPEND ATen_ATTENTION_KERNEL_SRCS ${flash_attention_cuda_kernels_cu})
diff --git a/caffe2/CMakeLists.txt b/caffe2/CMakeLists.txt
index a6b6f0f7d1..7d2f5daae3 100644
--- a/caffe2/CMakeLists.txt
+++ b/caffe2/CMakeLists.txt
@@ -1023,6 +1023,7 @@ elseif(USE_CUDA)
   torch_compile_options(torch_cuda)  # see cmake/public/utils.cmake
   target_compile_options_if_supported(torch_cuda "-Wno-deprecated-copy")  # see cmake/public/utils.cmake
   target_compile_definitions(torch_cuda PRIVATE USE_CUDA)
+  target_link_libraries(torch_cuda PRIVATE flash_attention_cuda)
 
   if(USE_CUSPARSELT)
       target_link_libraries(torch_cuda PRIVATE torch::cusparselt)
@@ -1624,6 +1625,9 @@ if(USE_CUDA)
   target_link_libraries(
       torch_cuda PRIVATE ${Caffe2_CUDA_DEPENDENCY_LIBS})
 
+  target_include_directories(
+      flash_attention_cuda PRIVATE ${Caffe2_GPU_INCLUDE})
+
   # These public dependencies must go after the previous dependencies, as the
   # order of the libraries in the linker call matters here when statically
   # linking; libculibos and cublas must be last.
-- 
2.40.1

